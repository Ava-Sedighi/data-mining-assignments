# -*- coding: utf-8 -*-
"""HW3_Group9.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1KvkIO1p-lArjxrVnzjY-assX3NdGC7Zt
"""

# First we import the libraries we need
import pandas as pd
import numpy as np

"""## Question 1"""

# reading the file which contains the data
online_retail = pd.read_excel('Online Retail.xlsx')
online_retail.head()

online_retail.columns = ['invoice_No', 'stock_code', 'description', 'quantity', 'invoice_date', 'unit_price', 'cutomer_id', 'country']
online_retail.head()

online_retail.info()
# it shows we have missing values in description and customer_id.

# make a copy to select the attributes we need.
online_retail_copy = online_retail.copy()
# dropping customer_id beccause with invoice_No available, we do not need it.
online_retail_copy.drop(['cutomer_id'], axis=1, inplace=True)
online_retail_copy.drop(['country'], axis=1, inplace=True)
online_retail_copy

online_retail_copy['stock_code'] = online_retail_copy['stock_code'].astype(str).str.replace(r'[a-zA-Z]', '', regex=True)
online_retail_copy['invoice_No'] = online_retail_copy['invoice_No'].astype(str).str.replace(r'^[a-zA-Z]', '', regex=True)
online_retail_copy['invoice_date'] = pd.to_datetime(online_retail_copy['invoice_date'])
online_retail_copy.head()

online_retail_copy['description'] = online_retail_copy['description'].str.lower()
online_retail_copy['description'] = online_retail_copy['description'].str.strip()
invalid_desc = ['postage', 'adjustment', 'delivery', 'discount', 'manual']

online_retail_valid = online_retail_copy[~online_retail_copy['description'].isin(invalid_desc)]
online_retail_valid

online_retail_valid[online_retail_valid['description'].isnull()]

mask_missing_desc = online_retail_valid['description'].isnull() | online_retail_valid['description'].str.strip().eq('')

desc_mapping = (
    online_retail_valid[~mask_missing_desc].groupby('stock_code')['description']
    .agg(lambda x: x.value_counts().idxmax())
    .to_dict()
)
online_retail_valid.loc[mask_missing_desc, 'description'] = online_retail_valid.loc[mask_missing_desc, 'stock_code'].map(desc_mapping)

online_retail_valid[online_retail_valid['invoice_No']=='581408']

online_retail_valid.info()

online_retail_valid= online_retail_valid.dropna(subset=['description']).reset_index(drop=True)
online_retail_valid.info()

online_retail_valid = online_retail_valid[online_retail_valid['quantity']>0]
online_retail_valid

"""### Section 1

"""

# First we create purchases and put the items purchased together for a single cart.
basket = online_retail_valid.groupby(['invoice_No', 'invoice_date'])['description'].apply(lambda x: list(set(x))).reset_index(name='products').reindex(columns=['invoice_date', 'invoice_No', 'products'])
basket

from mlxtend.frequent_patterns import apriori

partitions = np.array_split(basket, 4)

# 2. تابع کمکی برای one-hot encoding و Apriori در هر پارتیشن
def get_local_freq(df_part, min_support=0.5):
    # تبدیل لیست کالاها به ماتریس یک‌گره‌ای
    ohe = df_part['products'].str.join('|').str.get_dummies('|')
    # اجرای Apriori
    return apriori(ohe, min_support=min_support, use_colnames=True)

# استخراج آیتم‌ست‌های مکرر محلی
local_freq_list = [get_local_freq(p, min_support=0.0005) for p in partitions]
local_freq_list

# 3. ادغام همه‌ی آیتم‌ست‌های مکرر محلی برای ساخت نامزدهای کلی
candidates = pd.concat(local_freq_list)['itemsets'].drop_duplicates().tolist()

# 4. اسکن نهایی: شمارش نامزدها روی کل داده
ohe_full = basket['products'].str.join('|').str.get_dummies('|')
# محاسبه‌ی فرکانس واقعی نامزدها
global_counts = []
n_transactions = len(basket)
for itemset in candidates:
    mask = ohe_full[list(itemset)].all(axis=1)
    support = mask.sum() / n_transactions
    if support >= 0.0005:
        global_counts.append((frozenset(itemset), support))

# تبدیل به DataFrame
freq_itemsets = pd.DataFrame(global_counts, columns=['itemset','support']) \
                  .sort_values('support', ascending=False).reset_index(drop=True)

print(freq_itemsets)

"""### Section 2"""

from mlxtend.frequent_patterns import association_rules

# تغییر نام ستون itemset به itemsets
freq_itemsets = freq_itemsets.rename(columns={'itemset': 'itemsets'})

# حالا می‌توانیم قواعد انجمنی را استخراج کنیم
rules = association_rules(freq_itemsets, metric="confidence", min_threshold=0.0005)

# مرتب‌سازی و نمایش
rules = rules.sort_values(['confidence', 'lift'], ascending=[False, False]).reset_index(drop=True)
print(rules[['antecedents','consequents','support','confidence','lift']])

"""### Section 3"""

# 1) برترین آیتم‌ست‌ها بر اساس support (اندازه ≥ 2)
top_by_support = (freq_itemsets[
                    freq_itemsets['itemsets'].apply(lambda s: len(s) >= 2)]
                  .sort_values('support', ascending=False)
                  .head(5))
print("Top 5 itemsets by support:")
print(top_by_support)

# 2) برترین قوانین بر اساس lift
top_by_lift = (rules[
                 rules['antecedents'].apply(lambda s: len(s) >= 1) &
                 rules['consequents'].apply(lambda s: len(s) >= 1)]
               .sort_values('lift', ascending=False)
               .head(5)[['antecedents','consequents','support','confidence','lift']])
print("\nTop 5 association rules by lift:")
print(top_by_lift)

# 3) استخراج پیشنهاد ترکیب تخفیف گروهی
#   می‌توانیم از top_by_support یا ترکیب های antecedent+consequent در top_by_lift استفاده کنیم
bundle_suggestions = []
for _, row in top_by_lift.iterrows():
    combo = set(row['antecedents']) | set(row['consequents'])
    bundle_suggestions.append((frozenset(combo), row['lift'], row['confidence']))

print("\nBundle suggestions (from top rules):")
for combo, lift, conf in bundle_suggestions:
    print(f"{set(combo)}  →  lift={lift:.2f}, confidence={conf:.2f}")

"""### Section 4"""

basket['weekday'] = pd.to_datetime(basket['invoice_date']).dt.weekday
ohe_full= basket['products'].str.join('|').str.get_dummies('|')
weekend_mask = basket['weekday'].isin([5,6])
weekday_mask = ~weekend_mask

b_parts = np.array_split(basket, 4)
o_parts = np.array_split(ohe_full, 4)

local_wknd = []
local_wkdy = []

for b, o in zip(b_parts, o_parts):
    m_wknd = b['weekday'].isin([5,6])
    wknd_sets = apriori(o[m_wknd], min_support=0.0005, use_colnames=True, max_len=2)['itemsets']
    wkdy_sets = apriori(o[~m_wknd], min_support=0.0005, use_colnames=True, max_len=2)['itemsets']
    local_wknd.append({frozenset(s) for s in wknd_sets if len(s)==2})
    local_wkdy.append({frozenset(s) for s in wkdy_sets if len(s)==2})

# 3) ادغام نامزدهای کلی
cand_wknd = set.union(*local_wknd)
cand_wkdy = set.union(*local_wkdy)

# 4) شمارش نهاییِ support روی کل داده
n_wknd = weekend_mask.sum()
n_wkdy = weekday_mask.sum()

global_wknd = {
    p for p in cand_wknd
    if (ohe_full[list(p)][weekend_mask].all(axis=1).sum() / n_wknd) >= 0.0005
}
global_wkdy = {
    p for p in cand_wkdy
    if (ohe_full[list(p)][weekday_mask].all(axis=1).sum() / n_wkdy) >= 0.0005
}

# 5) جفت‌های منحصراً آخر هفته
unique_weekend_pairs = global_wknd - global_wkdy

print("جفت‌های منحصراً آخر هفته:")
for p in unique_weekend_pairs:
    print(set(p))

"""## Question 2"""

weather_history = pd.read_csv('weatherHistory.csv')
weather_history.head()

weather_history.info()

weather_history.describe()

from mlxtend.preprocessing import TransactionEncoder
from mlxtend.frequent_patterns import fpgrowth

"""### Section 1"""

weather_history_q1 = weather_history.copy()

weather_history_q1['high_temp'] = weather_history_q1['Temperature (C)'] >= 30    # دمای ≥ 30°C
weather_history_q1['high_hum']  = weather_history_q1['Humidity'] >= 0.8           # رطوبت ≥ 80%

transactions = weather_history_q1.apply(
    lambda row: ['high_temp'] if row['high_temp'] else []
                + ['high_hum']  if row['high_hum']  else [],
    axis=1
).tolist()
transactions

te = TransactionEncoder()
te_ary = te.fit(transactions).transform(transactions)
weather_history_te = pd.DataFrame(te_ary, columns=te.columns_)

# 5. اجرای الگوریتم FPGrowth با آستانه حمایت ۱٪
frequent_itemsets = fpgrowth(weather_history_te, min_support=0.002, use_colnames=True)

# 6. چاپ نتایج
print(frequent_itemsets)

"""### Section 2"""

weather_history_q2 = weather_history.copy()

def make_transaction(row):
    items = []
    items.append(str(row['Summary']))
    if pd.notna(row['Precip Type']):
        items.append(str(row['Precip Type']))
    return items

transactions_q2 = weather_history_q2.apply(make_transaction, axis=1).tolist()

transactions_q2

te_q2 = TransactionEncoder()
te_ary_q2 = te_q2.fit(transactions_q2).transform(transactions_q2)
weather_history_te_q2 = pd.DataFrame(te_ary_q2, columns=te_q2.columns_)

# 4. اجرای FPGrowth با حداقل حمایت 0.2
min_support = 0.002
frequent_itemsets_q2 = fpgrowth(weather_history_te_q2, min_support=min_support, use_colnames=True)

frequent_itemsets_q2

"""## Question 3"""

import pandas as pd
from sklearn.metrics import confusion_matrix
from scipy.stats import chi2_contingency

item_tid = {
    'T1': {101, 203, 305, 407, 509, 612, 714, 816, 919, 1020},
    'T2': {101, 305, 509, 612, 816, 919},
    'T3': {203, 407, 612, 714, 816},
    'T4': {1020, 305, 407, 612, 714},
    'T5': {101, 203, 305, 509},
    'T6': {612, 714, 816, 919, 1020},
    'T7': {203, 407, 509, 612, 714},
    'T8': {101, 305, 816, 919},
    'T9': {509, 612, 714, 1020},
    'T10': {203, 305, 407, 612},
    'T11': {101, 816, 919, 1020},
    'T12': {305, 407, 509, 714},
    'T13': {612, 714, 816, 919},
    'T14': {101, 203, 305, 509},
    'T15': {305, 509, 714, 919},
}

treatment_costs = {
    'T1': 150, 'T2': 200, 'T3': 75, 'T4': 120, 'T5': 90,
    'T6': 180, 'T7': 85, 'T8': 250, 'T9': 95, 'T10': 110,
    'T11': 300, 'T12': 80, 'T13': 160, 'T14': 170, 'T15': 300
}

"""### A"""

df = pd.read_excel('Q3_Data.xlsx', sheet_name='Sheet2')
T1_patients = {101, 203, 305, 407, 509, 612, 714, 816, 919, 1020}
df['T1'] = df['TID'].apply(lambda x: 1 if x in T1_patients else 0)

contingency_table = pd.crosstab(df['T1'], df['Readmission'])


N00 = contingency_table.loc[0, 0] if (0 in contingency_table.index and 0 in contingency_table.columns) else 0
N01 = contingency_table.loc[0, 1] if (0 in contingency_table.index and 1 in contingency_table.columns) else 0
N10 = contingency_table.loc[1, 0] if (1 in contingency_table.index and 0 in contingency_table.columns) else 0
N11 = contingency_table.loc[1, 1] if (1 in contingency_table.index and 1 in contingency_table.columns) else 0


P_readmit = (N01 + N11) / (N00 + N01 + N10 + N11)
P_T1_given_readmit = N11 / (N11 + N01) if (N11 + N01) > 0 else 0
P_T1 = (N10 + N11) / (N00 + N01 + N10 + N11)

lift = P_T1_given_readmit / P_T1 if P_T1 > 0 else 0


chi2, p_val, dof, expected = chi2_contingency(contingency_table, correction=False)


print("Contingency Table:")
print(contingency_table)
print(f"\nLift for T1: {lift:.4f}")
print(f"Chi-square Statistic: {chi2:.4f}")
print(f"P-value: {p_val:.4f}")

"""### B"""

frequent_1_itemsets = {item for item, tids in item_tid.items() if len(tids) >= 5}
print("Frequent 1-itemsets (support >= 5):")
print(frequent_1_itemsets)

# 2-item
from itertools import combinations

frequent_2_itemsets = {}

for item1, item2 in combinations(frequent_1_itemsets, 2):
    tids1 = item_tid[item1]
    tids2 = item_tid[item2]
    intersection = tids1 & tids2
    if len(intersection) >= 5:
        frequent_2_itemsets[(item1, item2)] = intersection

print("\nFrequent 2-itemsets:")
for pair, tids in frequent_2_itemsets.items():
    print(f"{pair} => support: {len(tids)}")


# 3-item
frequent_3_itemsets = {}

for item1, item2, item3 in combinations(frequent_1_itemsets, 3):
    tids1 = item_tid[item1]
    tids2 = item_tid[item2]
    tids3 = item_tid[item3]
    intersection = tids1 & tids2 & tids3
    if len(intersection) >= 4:
        frequent_3_itemsets[(item1, item2, item3)] = intersection

print("\nFrequent 3-itemsets:")
for triple, tids in frequent_3_itemsets.items():
    print(f"{triple} => support: {len(tids)}")

"""### C"""

from collections import defaultdict

tid_items = defaultdict(set)
for treatment, tids in item_tid.items():
    for tid in tids:
        tid_items[tid].add(treatment)

tid_costs = {
    tid: sum(treatment_costs[t] for t in treatments)
    for tid, treatments in tid_items.items()
}


for tid, cost in tid_costs.items():
    print(f"Patient {tid}: Total cost = ${cost}")

"""## Question 5"""

# first we read the datas.
movies_df = pd.read_csv("movies.csv")
movies_df.head()

movies_df.columns = ['movie_id', 'title', 'genres'] # Then rewriting the columns' names in a more standard way.

movies_df.info()  # Getting information to get to know the data better.

# first we read the datas.
ratings_df = pd.read_csv("ratings.csv")
ratings_df.head()

ratings_df.columns = ['user_id', 'movie_id', 'rating', 'time_stamp']   # Then rewriting the columns' names in a more standard way.

ratings_df.info()   # Getting information to get to know the data better.

from datetime import datetime
# Adding date time and the date
ratings_df['datetime'] = pd.to_datetime(ratings_df['time_stamp'], unit='s')
ratings_df['date'] = ratings_df['datetime'].dt.date
ratings_df.head()

ratings_df['day_name'] = ratings_df['datetime'].dt.day_name()
ratings_df['day_type'] = ratings_df['day_name'].apply(
    lambda x: 'weekend' if x in ['Friday', 'Saturday'] else 'weekday'
)
ratings_df.head()

"""### Section 1"""

merged_df = ratings_df.merge(movies_df[['movie_id', 'genres']], on='movie_id', how='left')
transactions = merged_df.groupby(['user_id', 'date', 'day_type'])['genres'].apply(list).reset_index()

def has_family_or_children(genres_list):
    return any(('Family' in genre or 'Children' in genre) for genre in genres_list)

def has_documentary_or_educational(genres_list):
    return any(('Documentary' in genre or 'Educational' in genre) for genre in genres_list)

from mlxtend.preprocessing import TransactionEncoder
from mlxtend.frequent_patterns import fpgrowth

weekday_transactions = transactions[(transactions['day_type'] == 'weekday') &
                                     (transactions['genres'].apply(has_documentary_or_educational))]

weekend_transactions = transactions[(transactions['day_type'] == 'weekend') &
                                     (transactions['genres'].apply(has_family_or_children))]

genres = ['Family', 'Children', 'Documentary', 'Educational', 'Drama', 'Comedy', 'Adventure']

te_weekday = TransactionEncoder()
te_array_weekday = te_weekday.fit(weekday_transactions['genres']).transform(weekday_transactions['genres'])
df_weekday = pd.DataFrame(te_array_weekday, columns=te_weekday.columns_)
df_weekday = df_weekday[[col for col in genres if col in df_weekday.columns]]

te_weekend = TransactionEncoder()
te_array_weekend = te_weekend.fit(weekend_transactions['genres']).transform(weekend_transactions['genres'])
df_weekend = pd.DataFrame(te_array_weekend, columns=te_weekend.columns_)
df_weekend = df_weekend[[col for col in genres if col in df_weekend.columns]]

frequent_patterns_weekday = fpgrowth(df_weekday, min_support=0.05, use_colnames=True)

frequent_patterns_weekend = fpgrowth(df_weekend, min_support=0.05, use_colnames=True)

print(frequent_patterns_weekend)

print(frequent_patterns_weekday)

"""### Section 2"""

from mlxtend.frequent_patterns import association_rules

# Weekend rules
weekend_rules = association_rules(frequent_patterns_weekend, metric="confidence", min_threshold=0.65)
filtered_weekend_rules = weekend_rules[weekend_rules.lift >= 1.3]
top5_weekend = filtered_weekend_rules.nlargest(5, 'lift')

print("Top 5 Association Rules - Weekend:")
print(top5_weekend[['antecedents', 'consequents', 'support', 'confidence', 'lift']])

# Weekday rules
weekday_rules = association_rules(frequent_patterns_weekday, metric="confidence", min_threshold=0.65)
filtered_weekday_rules = weekday_rules[weekday_rules.lift >= 1.3]
top5_weekday = filtered_weekday_rules.nlargest(5, 'lift')

print("Top 5 Association Rules - Weekday:")
print(top5_weekday[['antecedents', 'consequents', 'support', 'confidence', 'lift']])

"""### Section 3"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

frequent_patterns_weekend['pattern_size'] = frequent_patterns_weekend['itemsets'].apply(len)
frequent_patterns_weekday['pattern_size'] = frequent_patterns_weekday['itemsets'].apply(len)

def plot_pattern_sizes(df, title, color, ax):
    counts = df['pattern_size'].value_counts().sort_index()
    ax.bar(counts.index, counts.values, color=color)
    ax.set_title(title)
    ax.set_xlabel('Size')
    ax.set_ylabel('Frequency')

fig, axes = plt.subplots(1, 2, figsize=(10, 4))

plot_pattern_sizes(frequent_patterns_weekend, 'Weekend - Pattern Size', '#a01a58', axes[0])
plot_pattern_sizes(frequent_patterns_weekday, 'Weekday - Pattern Size', '#1780a1', axes[1])

plt.tight_layout()
plt.show()

"""#### Heatmap"""

import numpy as np
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt

genres = ['Documentary', 'Drama', 'Comedy']

weekend_data = [
    [0.17, 0.16, 0.15],
    [0.16, 0.60, 0.46],
    [0.15, 0.46, 0.57]
]

weekday_data = [
    [0.79, 0.51, 0.45],
    [0.51, 0.63, 0.49],
    [0.45, 0.49, 0.56]
]

weekend_df = pd.DataFrame(weekend_data, index=genres, columns=genres)
weekday_df = pd.DataFrame(weekday_data, index=genres, columns=genres)

def remove_diag(df):
    df = df.copy()
    np.fill_diagonal(df.values, 0)
    return df

def plot_heatmap(df, title):
    plt.figure(figsize=(6, 5))
    sns.heatmap(df, annot=True, cmap='coolwarm', square=True)
    plt.title(title)
    plt.tight_layout()
    plt.show()

plot_heatmap(remove_diag(weekend_df), "Genre Co-occurrence Heatmap - Weekend")
plot_heatmap(remove_diag(weekday_df), "Genre Co-occurrence Heatmap - Weekday")